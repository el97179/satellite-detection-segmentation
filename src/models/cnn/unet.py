"""
U-Net implementation for satellite segmentation.

This module implements a U-Net architecture optimized for satellite detection
and segmentation tasks, with support for multi-class segmentation and 
flexible input/output configurations.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class DoubleConv(nn.Module):
    """
    Double convolution block used in U-Net encoder and decoder.
    
    Consists of two 3x3 convolutions, each followed by batch normalization
    and ReLU activation.
    """
    
    def __init__(self, in_channels: int, out_channels: int, dropout_rate: float = 0.0):
        """
        Initialize double convolution block.
        
        Args:
            in_channels: Number of input channels
            out_channels: Number of output channels
            dropout_rate: Dropout rate (0.0 = no dropout)
        """
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity(),
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through double convolution."""
        return self.conv(x)


class Down(nn.Module):
    """
    Downsampling block for U-Net encoder.
    
    Consists of max pooling followed by double convolution.
    """
    
    def __init__(self, in_channels: int, out_channels: int, dropout_rate: float = 0.0):
        """
        Initialize downsampling block.
        
        Args:
            in_channels: Number of input channels
            out_channels: Number of output channels
            dropout_rate: Dropout rate for convolution blocks
        """
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels, dropout_rate)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through downsampling block."""
        return self.maxpool_conv(x)


class Up(nn.Module):
    """
    Upsampling block for U-Net decoder.
    
    Consists of upsampling (transpose convolution or bilinear interpolation)
    followed by concatenation with skip connection and double convolution.
    """
    
    def __init__(
        self, 
        in_channels: int, 
        out_channels: int, 
        bilinear: bool = True,
        dropout_rate: float = 0.0
    ):
        """
        Initialize upsampling block.
        
        Args:
            in_channels: Number of input channels
            out_channels: Number of output channels
            bilinear: Use bilinear upsampling instead of transpose convolution
            dropout_rate: Dropout rate for convolution blocks
        """
        super().__init__()
        
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, dropout_rate)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, 2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels, dropout_rate)
    
    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through upsampling block.
        
        Args:
            x1: Input from previous decoder layer
            x2: Skip connection from encoder
            
        Returns:
            Concatenated and processed features
        """
        x1 = self.up(x1)
        
        # Handle size mismatch between x1 and x2
        diff_y = x2.size()[2] - x1.size()[2]
        diff_x = x2.size()[3] - x1.size()[3]
        
        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,
                        diff_y // 2, diff_y - diff_y // 2])
        
        # Concatenate skip connection
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class AttentionGate(nn.Module):
    """
    Attention gate module for focusing on relevant features.
    
    Implements attention mechanism to weight skip connections
    based on relevance to current decoder features.
    """
    
    def __init__(self, gate_channels: int, in_channels: int, inter_channels: int):
        """
        Initialize attention gate.
        
        Args:
            gate_channels: Number of channels in gate signal
            in_channels: Number of channels in input signal
            inter_channels: Number of intermediate channels
        """
        super().__init__()
        
        self.W_gate = nn.Sequential(
            nn.Conv2d(gate_channels, inter_channels, 1, bias=True),
            nn.BatchNorm2d(inter_channels)
        )
        
        self.W_x = nn.Sequential(
            nn.Conv2d(in_channels, inter_channels, 1, bias=True),
            nn.BatchNorm2d(inter_channels)
        )
        
        self.psi = nn.Sequential(
            nn.Conv2d(inter_channels, 1, 1, bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, gate: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through attention gate.
        
        Args:
            gate: Gate signal from decoder
            x: Input signal from encoder
            
        Returns:
            Attention-weighted features
        """
        gate_proj = self.W_gate(gate)
        x_proj = self.W_x(x)
        
        # Resize gate to match x if necessary
        if gate_proj.size() != x_proj.size():
            gate_proj = F.interpolate(gate_proj, size=x_proj.shape[2:], 
                                    mode='bilinear', align_corners=False)
        
        attention = self.relu(gate_proj + x_proj)
        attention = self.psi(attention)
        
        return x * attention


class UNet(nn.Module):
    """U-Net architecture for satellite segmentation.

    NOTE: Original implementation in this repository was detection-oriented and
    returned anchor-based predictions. The test suite (and typical U-Net usage)
    expects a semantic segmentation map of shape (B, C, H, W). This refactored
    version restores classic segmentation behaviour while keeping backward
    compatibility with previous parameter names (``num_classes`` and
    ``n_classes``).
    """

    def __init__(
        self,
        n_channels: int = 3,
        n_classes: int = 1,
        # Backward compatibility: allow ``num_classes`` alias
        num_classes: int = None,  # type: ignore[assignment]
        base_channels: int = 64,
        depth: int = 4,
        bilinear: bool = True,
        use_attention: bool = False,
        dropout_rate: float = 0.1,
    ):
        super().__init__()
        if num_classes is not None:  # alias support
            n_classes = num_classes
        self.n_channels = n_channels
        self.num_classes = n_classes
        self.depth = depth
        self.use_attention = use_attention

        # Channel sizes encoderâ†’decoder
        channels = [base_channels * (2 ** i) for i in range(depth + 1)]

        # Encoder
        self.inc = DoubleConv(n_channels, channels[0], dropout_rate)
        self.downs = nn.ModuleList(
            [Down(channels[i], channels[i + 1], dropout_rate) for i in range(depth)]
        )

        # Attention gates (optional)
        if use_attention:
            self.attention_gates = nn.ModuleList()
            for i in range(depth):
                gate_ch = channels[depth - i]
                in_ch = channels[depth - i - 1]
                inter_ch = channels[depth - i - 1] // 2
                self.attention_gates.append(AttentionGate(gate_ch, in_ch, inter_ch))

        # Decoder
        self.ups = nn.ModuleList()
        for i in range(depth):
            in_ch = channels[depth - i] + channels[depth - i - 1]
            out_ch = channels[depth - i - 1]
            self.ups.append(Up(in_ch, out_ch, bilinear, dropout_rate))

        # Segmentation head
        self.outc = nn.Conv2d(channels[0], n_classes, 1)

        self._initialize_weights()
        logger.info(
            "Initialized U-Net Segmentation: in_channels=%d, classes=%d, depth=%d, attention=%s",
            n_channels, n_classes, depth, use_attention,
        )
    
    def _initialize_weights(self):
        """Initialize model weights using Xavier initialization."""
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass returning segmentation logits of shape (B, C, H, W)."""
        x1 = self.inc(x)
        skip_connections = [x1]
        x_current = x1
        for down in self.downs:
            x_current = down(x_current)
            skip_connections.append(x_current)
        x_current = skip_connections[-1]
        for i, up in enumerate(self.ups):
            skip_idx = len(skip_connections) - 2 - i
            skip_conn = skip_connections[skip_idx]
            if self.use_attention:
                skip_conn = self.attention_gates[i](x_current, skip_conn)
            x_current = up(x_current, skip_conn)
        return self.outc(x_current)
    
    def get_num_parameters(self) -> int:
        """Get total number of trainable parameters."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class UNetWithAuxiliaryOutputs(UNet):
    """U-Net with auxiliary segmentation outputs (deep supervision)."""

    def __init__(self, *args, aux_weight: float = 0.4, **kwargs):
        super().__init__(*args, **kwargs)
        self.aux_weight = aux_weight
        # Build auxiliary heads for intermediate decoder outputs (exclude final)
        self.aux_heads = nn.ModuleList()
        # Reconstruct channel progression used previously
        # (base_channels not stored directly; infer from first down block)
        # For simplicity derive from first conv in inc
        # This is a best-effort approach; auxiliary heads only used in tests for shape.
        # We'll collect out_channels of each Up module.
        for i in range(len(self.ups) - 1):
            out_ch = self.ups[i].conv.conv[0].out_channels  # type: ignore[index]
            self.aux_heads.append(nn.Conv2d(out_ch, self.num_classes, 1))

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        x1 = self.inc(x)
        skip_connections = [x1]
        x_current = x1
        for down in self.downs:
            x_current = down(x_current)
            skip_connections.append(x_current)
        x_current = skip_connections[-1]
        aux_outputs: List[torch.Tensor] = []
        for i, up in enumerate(self.ups):
            skip_idx = len(skip_connections) - 2 - i
            skip_conn = skip_connections[skip_idx]
            if self.use_attention:
                skip_conn = self.attention_gates[i](x_current, skip_conn)  # type: ignore[attr-defined]
            x_current = up(x_current, skip_conn)
            if i < len(self.ups) - 1:
                aux_logits = self.aux_heads[i](x_current)
                # Upsample auxiliary logits to final resolution for test expectations
                # (tests expect identical shape to main output)
                aux_outputs.append(nn.functional.interpolate(
                    aux_logits, scale_factor=2 ** (len(self.ups) - 1 - i), mode='bilinear', align_corners=False
                ))
        main_output = self.outc(x_current)
        return main_output, aux_outputs


def create_unet_model(config: dict) -> nn.Module:
    """
    Create U-Net model from configuration.
    
    Args:
        config: Model configuration dictionary
        
    Returns:
        Configured U-Net model for detection
    """
    model_config = config.get('model', {})
    unet_config = model_config.get('unet', {})
    
    # Default parameters
    # Support both 'num_classes' (new) and 'n_classes' (legacy tests) keys.
    n_classes = unet_config.get('num_classes', unet_config.get('n_classes', 1))
    params = {
        'n_channels': unet_config.get('n_channels', 3),
        'n_classes': n_classes,
        'base_channels': unet_config.get('base_channels', 64),
        'depth': unet_config.get('depth', 4),
        'bilinear': unet_config.get('bilinear', True),
        'use_attention': unet_config.get('use_attention', False),
        'dropout_rate': unet_config.get('dropout_rate', 0.1),
    }
    
    # Check if auxiliary outputs are requested
    use_aux = unet_config.get('use_auxiliary_outputs', False)
    
    if use_aux:
        aux_weight = unet_config.get('auxiliary_weight', 0.4)
        unet_model = UNetWithAuxiliaryOutputs(**params, aux_weight=aux_weight)
    else:
        unet_model = UNet(**params)
    
    logger.info("Created U-Net model with %d parameters", unet_model.get_num_parameters())
    return unet_model


if __name__ == "__main__":
    # Example usage and testing
    logging.basicConfig(level=logging.INFO)
    
    # Test basic U-Net
    model = UNet(n_channels=3, num_classes=2, depth=4, use_attention=True)
    
    # Test input
    x = torch.randn(2, 3, 256, 256)
    
    print(f"Input shape: {x.shape}")
    print(f"Model parameters: {model.get_num_parameters():,}")
    
    # Forward pass
    with torch.no_grad():
        output = model(x)
        print(f"Output shape: {output.shape}")
    
    # Test auxiliary outputs model
    aux_model = UNetWithAuxiliaryOutputs(n_channels=3, n_classes=2, depth=4)
    
    with torch.no_grad():
        main_out, aux_outs = aux_model(x)
        print(f"Main output shape: {main_out.shape}")
        print(f"Number of auxiliary outputs: {len(aux_outs)}")
        for i, aux_out in enumerate(aux_outs):
            print(f"  Aux output {i} shape: {aux_out.shape}")